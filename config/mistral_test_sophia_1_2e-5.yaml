data:
  train_urls:
    - "gs://levanter-dataset-eu/NCC_plus_scandi/train-shard-{0001..0147}-of-0147.json.gz"
  validation_urls:
    - "gs://levanter-dataset-eu/NCC_plus_scandi/validation-shard-0001-of-0001.json.gz"
  cache_dir: "gs://levanter-cache-eu/NCC_plus_scandi/tokenized/llama/"
  tokenizer: "mistralai/Mistral-7B-v0.1"
model:
  type: mistral
  seq_len: 2048
  hidden_dim: 4096
  intermediate_dim: 14336
  num_layers: 32
  num_heads: 32
  num_kv_heads: 8
  activation_function: silu
  initializer_range: 0.02
  layer_norm_epsilon: 1.0e-05
  upcast_attn: false
  use_flash_attention: true
  flash_attention_block_size: null
  scan_layers: true
  use_bias: false
  rope_scaling: null
  sliding_window: 4096
trainer:
  wandb:
    entity: "nbailab"
    project: "north-mistral"
    tags: ["openwebtext", "mistral"]
    name: north-mistral-sophia_1_2e_5
  mp: p=f32,c=bfloat16
  train_batch_size: 512  # 1024 working v5e-128, set 256 for v4-64 TPU
  num_train_steps: 10000
  steps_per_eval: 250
  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed"
  batch_axis: "batch"
  checkpointer:
    base_path: "gs://levanter-checkpoint-eu/north-mistral-7b-defaulttoken-sophia-1-2e-5/checkpoints"
    keep:
      - every: 1000
initialize_from_hf: "mistralai/Mistral-7B-v0.1"
use_hf_model_config: false
hf_save_steps: 5000
hf_save_path: "gs://levanter-checkpoint-eu/north-mistral-7b-defaulttoken-sophia-1-2e-5/hf"
optimizer:
  type: sophia-h
  learning_rate: 1.2E-5
  weight_decay: 0.2
  min_lr_ratio: 0.1
  gamma: 0.01
  # sophia needs a minimum amount of warmup or it doesn't do well
  warmup: 2000
  #learning_rate: 1.2E-5  # set low for fine-tuning
  #weight_decay: 0.1
  #min_lr_ratio: 0.1
  #lr_schedule: cosine
  #warmup: 0.01
